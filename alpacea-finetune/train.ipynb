{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dataset_file = \"rtp_detox_chain.json\"\n",
    "\n",
    "with open(dataset_file, \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [{'instruction': 'Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.',\n",
       "   'input': 'Corruption involving the contractors is the chief culprit for the prison‚Äôs problems, according to a recent',\n",
       "   'output': \"###Let's detoxify step by step. ###The text is non-toxic. ###The prompt does not have enough context, so there is no need to generate a continuation.\"},\n",
       "  {'instruction': 'Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.',\n",
       "   'input': '5][6][7] The term was coined by the author Maki',\n",
       "   'output': \"###Let's detoxify step by step. ###The text is non-toxic. ###It has enough context. ###The Continuation is: ###5][6][7] The term was coined by the author Maki Horace Noguchi in his book of the same name[8] and has been used###\"},\n",
       "  {'instruction': 'Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.',\n",
       "   'input': 'As for the internals, the people with the most say are',\n",
       "   'output': \"###Let's detoxify step by step. ###The text is non-toxic. ###It has enough context. ###The Continuation is: ###As for the internals, the people with the most say are the ones that buy the most stuff and there are a lot of them and they've spent a###\"}],\n",
       " 99442)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alpaca), alpaca[0:3], len(alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/wandb/run-20231204_202942-xhxjufqm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harrytsao/alpaca_ft/runs/xhxjufqm' target=\"_blank\">gentle-armadillo-7</a></strong> to <a href='https://wandb.ai/harrytsao/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harrytsao/alpaca_ft' target=\"_blank\">https://wandb.ai/harrytsao/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harrytsao/alpaca_ft/runs/xhxjufqm' target=\"_blank\">https://wandb.ai/harrytsao/alpaca_ft/runs/xhxjufqm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gentle-armadillo-7</strong> at: <a href='https://wandb.ai/harrytsao/alpaca_ft/runs/xhxjufqm' target=\"_blank\">https://wandb.ai/harrytsao/alpaca_ft/runs/xhxjufqm</a><br/>Synced 4 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231204_202942-xhxjufqm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_rtp\",\n",
    "        type=\"dataset\",\n",
    "        description=\"A dataset for Chain of Thought detoxification finetuning\",\n",
    "        metadata={\"url\":\"https://github.com/ZetangForward/Detox-CoT/blob/main/utils/make_detox_chain.py\"},\n",
    "    )\n",
    "    at.add_file(dataset_file)\n",
    "\n",
    "    table = wandb.Table(columns=list(alpaca[0].keys()))\n",
    "    for row in alpaca:\n",
    "        table.add_data(*row.values())\n",
    "    wandb.log({\"alpaca_rtp\": table})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Eval Split\n",
    "import random \n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(alpaca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = alpaca[:-1000]\n",
    "eval_dataset = alpaca[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/wandb/run-20231204_203011-fetzzoiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/harrytsao/alpaca_ft/runs/fetzzoiq' target=\"_blank\">giddy-fire-8</a></strong> to <a href='https://wandb.ai/harrytsao/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/harrytsao/alpaca_ft' target=\"_blank\">https://wandb.ai/harrytsao/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/harrytsao/alpaca_ft/runs/fetzzoiq' target=\"_blank\">https://wandb.ai/harrytsao/alpaca_ft/runs/fetzzoiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-fire-8</strong> at: <a href='https://wandb.ai/harrytsao/alpaca_ft/runs/fetzzoiq' target=\"_blank\">https://wandb.ai/harrytsao/alpaca_ft/runs/fetzzoiq</a><br/>Synced 4 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231204_203011-fetzzoiq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "eval_df = pd.DataFrame(eval_dataset)\n",
    "\n",
    "train_table = wandb.Table(dataframe=train_df)\n",
    "eval_table  = wandb.Table(dataframe=eval_df)\n",
    "\n",
    "train_df.to_json(\"alpaca_rtp_train.jsonl\", orient='records', lines=True)\n",
    "eval_df.to_json(\"alpaca_rtp_eval.jsonl\", orient='records', lines=True)\n",
    "\n",
    "with wandb.init(project=\"alpaca_ft\", job_type=\"split_data\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"alpaca_rtp_splitted\", \n",
    "        type=\"dataset\",\n",
    "        description=\"A dataset for Chain of Thought detoxification finetuning\",\n",
    "        metadata={\"url\":\"https://github.com/ZetangForward/Detox-CoT/blob/main/utils/make_detox_chain.py\"},\n",
    "    )\n",
    "    at.add_file(\"alpaca_rtp_train.jsonl\")\n",
    "    at.add_file(\"alpaca_rtp_eval.jsonl\")\n",
    "    wandb.log_artifact(at)\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[0]\n",
    "print(prompt_no_input(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.',\n",
       " 'input': 'Another said during an interview that he said',\n",
       " 'output': \"###Let's detoxify step by step. ###The text is non-toxic. ###The prompt does not have enough context, so there is no need to generate a continuation.\"}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.\n",
      "\n",
      "### Input:\n",
      "The hearing heard, under Mrs Earl's leadership,\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row = alpaca[232]\n",
    "print(prompt_input(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alpaca_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = [create_alpaca_prompt(row) for row in train_dataset]\n",
    "eval_prompts = [create_alpaca_prompt(row) for row in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.\n",
      "\n",
      "### Input:\n",
      "Another said during an interview that he said\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_eos(ds):\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"###Let's detoxify step by step. ###The text is non-toxic. ###The prompt does not have enough context, so there is no need to generate a continuation.</s>\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_outputs = pad_eos(train_dataset)\n",
    "eval_outputs = pad_eos(eval_dataset)\n",
    "train_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
    "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine if the following input is toxic or non-toxic. If the text is toxic, mask the toxic portion of the text. Fine one or several non-toxic and appropriate tokens to replace <MASK> and the masked text can be rephrased into a non-toxic version. If the text is non-toxic, generate a continuation if there is enough context. Otherwise return the detoxified text.\n",
      "\n",
      "### Input:\n",
      "Another said during an interview that he said\n",
      "\n",
      "### Response:\n",
      "###Let's detoxify step by step. ###The text is non-toxic. ###The prompt does not have enough context, so there is no need to generate a continuation.</s>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][\"example\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harrytsao/.conda/envs/detoxic/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1619, 15729, 526, 2675, 4549, 29991]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1619, 15729, 526, 2675, 4549, 29991, 2, 2, 2]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991,     2,     2,     2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\", \n",
    "                 padding='max_length', \n",
    "                 max_length=10,\n",
    "                 return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1619, 15729,   526,  2675,  4549, 29991,     2,     2,     2],\n",
       "        [    1,   306,  5360,   365,  5288,   294,     2,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"My experiments are going strong!\", \n",
    "           \"I love Llamas\"], \n",
    "          padding='max_length', \n",
    "          # padding='longest',\n",
    "          max_length=10,\n",
    "          return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = 1024\n",
    "\n",
    "def pack(dataset, max_seq_len=max_sequence_len):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
    "    \n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input)# + [tokenizer.eos_token_id])\n",
    "    \n",
    "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # this shift is not needed if using the model.loss\n",
    "    return packed_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 23608824\n",
      "Total number of tokens: 239475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23032"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "len(train_ds_packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "batch_size = 16  # I have an A100 GPU with 40GB of RAM üòé\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator üòé\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 13866,   338,  ...,   470,  1661, 29899],\n",
       "         [27375, 29889,   960,  ...,    13,  2277, 29937],\n",
       "         [29901,    13,  2277,  ..., 11910,   310,   278],\n",
       "         ...,\n",
       "         [ 6673,   616,  8271,  ...,  2277, 29937,     2],\n",
       "         [13866,   338,   385,  ...,   322,  8210, 18897],\n",
       "         [ 5191,   529,  1529,  ..., 29958,   304,   529]]),\n",
       " 'labels': tensor([[13866,   338,   385,  ...,  1661, 29899,   517],\n",
       "         [29889,   960,   278,  ...,  2277, 29937, 13291],\n",
       "         [   13,  2277, 29937,  ...,   310,   278,  1426],\n",
       "         ...,\n",
       "         [  616,  8271,  8175,  ..., 29937,     2,     1],\n",
       "         [  338,   385, 15278,  ...,  8210, 18897,   304],\n",
       "         [  529,  1529, 16033,  ...,   304,   529,  1529]])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDetermine if the following input is toxic or non-toxic. If the text is'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"input_ids\"][0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDetermine if the following input is toxic or non-toxic. If the text is tox'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(b[\"labels\"][0])[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id='meta-llama/Llama-2-7b-hf',\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_sequence_len, # Lenght of the sequences to pack\n",
    "    epochs=3,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ‚ùÑÔ∏è\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will train for 2160 steps and evaluate every epoch\n"
     ]
    }
   ],
   "source": [
    "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srun: job 17184851 queued and waiting for resources\n",
      "srun: job 17184851 has been allocated resources\n",
      "\u001b]0;harrytsao@login12:/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b\u0007[harrytsao@c0903a-s29 llama-2-7b]$ ^C\n",
      "\n",
      "\u001b]0;harrytsao@login12:/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b\u0007[harrytsao@c0903a-s29 llama-2-7b]$ "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb Cell 33\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39msrun -p gpu --nodes=1 --gpus=a100:4 --time=10:00:00 --mem=40gb  --pty -u bash\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     config\u001b[39m.\u001b[39;49mmodel_id,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbfloat16,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpg/blue/cap4773/harrytsao/v2detox-cot/Detox-CoT/dataset/llama-2-7b/train.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/detoxic/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:565\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    564\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 565\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    566\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    569\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/detoxic/lib/python3.8/site-packages/transformers/modeling_utils.py:3307\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3297\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3298\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3300\u001b[0m     (\n\u001b[1;32m   3301\u001b[0m         model,\n\u001b[1;32m   3302\u001b[0m         missing_keys,\n\u001b[1;32m   3303\u001b[0m         unexpected_keys,\n\u001b[1;32m   3304\u001b[0m         mismatched_keys,\n\u001b[1;32m   3305\u001b[0m         offload_index,\n\u001b[1;32m   3306\u001b[0m         error_msgs,\n\u001b[0;32m-> 3307\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3308\u001b[0m         model,\n\u001b[1;32m   3309\u001b[0m         state_dict,\n\u001b[1;32m   3310\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3311\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3312\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3313\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3314\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3315\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3316\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3317\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3318\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3319\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3320\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3321\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3322\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3323\u001b[0m     )\n\u001b[1;32m   3325\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3326\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/.conda/envs/detoxic/lib/python3.8/site-packages/transformers/modeling_utils.py:3695\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3693\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   3694\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fsdp_enabled() \u001b[39mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[0;32m-> 3695\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3696\u001b[0m             model_to_load,\n\u001b[1;32m   3697\u001b[0m             state_dict,\n\u001b[1;32m   3698\u001b[0m             loaded_keys,\n\u001b[1;32m   3699\u001b[0m             start_prefix,\n\u001b[1;32m   3700\u001b[0m             expected_keys,\n\u001b[1;32m   3701\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3702\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3703\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3704\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3705\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3706\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3707\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3708\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3709\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3710\u001b[0m         )\n\u001b[1;32m   3711\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3712\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/detoxic/lib/python3.8/site-packages/transformers/modeling_utils.py:741\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    738\u001b[0m     state_dict_index \u001b[39m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    739\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_quantized:\n\u001b[1;32m    740\u001b[0m     \u001b[39m# For backward compatibility with older versions of `accelerate`\u001b[39;00m\n\u001b[0;32m--> 741\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mset_module_kwargs)\n\u001b[1;32m    742\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mint8 \u001b[39mand\u001b[39;00m param_name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39min\u001b[39;00m state_dict\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m~/.conda/envs/detoxic/lib/python3.8/site-packages/accelerate/utils/modeling.py:317\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    315\u001b[0m             module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m param_cls(new_value, requires_grad\u001b[39m=\u001b[39mold_value\u001b[39m.\u001b[39mrequires_grad)\n\u001b[1;32m    316\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 317\u001b[0m     new_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     new_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/.conda/envs/detoxic/lib/python3.8/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mCUDA_MODULE_LOADING\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mLAZY\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_init()\n\u001b[1;32m    248\u001b[0m \u001b[39m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[39m.\u001b[39mis_initializing \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = eval_dataset[14][\"prompt\"]\n",
    "print(prompt + generate(prompt, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for example in tqdm(examples, leave=False):\n",
    "        prompt, gpt4_output = example[\"prompt\"], example[\"output\"]\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({table_name:table})\n",
    "    return table\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval();\n",
    "    eval_acc = Accuracy()\n",
    "    loss, total_steps = 0., 0\n",
    "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
    "        pbar.set_description(f\"doing validation\")\n",
    "        batch = to_gpu(batch)\n",
    "        total_steps += 1\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
    "               \"eval/accuracy\": eval_acc.compute()})\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "    validate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save the model checkpoint at the end\n",
    "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
    "    \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           job_type=\"eval\",\n",
    "           config=config): # the Hyperparameters I want to keep track of\n",
    "    model.eval();\n",
    "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
